# 国内网络
export https_proxy=http://183.174.229.137:3128
curl www.baidu.com
# 国外网络
export http_proxy=http://183.174.229.137:9877
curl www.google.com

tensorboard --logdir ./logs/[path of tensorboard file] --port=[eg:6008]
exchange
# tiny
bash run/vqa_finetune.bash 1 vqa_tiny --tiny --type base
bash run/vqa2_finetune.bash 1 vqa2_tiny_base --tiny --type base
bash run/vqa2_finetune.bash 1 vqa2_tiny_exchange --tiny --type exchange
bash run/vqa2_finetune.bash 1 vqa2_tiny_meanpooling --tiny --type meanpooling
bash run/vqa2_finetune.bash 1 vqa2_tiny_maxpooling --tiny --type maxpooling
# large  [--resume Last]
bash run/vqa_finetune.bash 0 vqa_src_base --type base
bash run/vqa2_finetune.bash 0 vqa2_large_base --type base
bash run/vqa2_finetune.bash 1 vqa2_large_exchange --type exchange
bash run/vqa2_finetune.bash 1 vqa2_large_meanpooling --type meanpooling
bash run/vqa2_finetune.bash 1 vqa2_large_maxpooling --type maxpooling

# Based on load
# bash run/vqa2_finetune_exchange2.bash 0 vqa2_large_base_exchange2 --type exchange2
# bash run/vqa2_finetune.bash 0 vqa2_large_exchange2 --type exchange2

# local validate
bash run/vqa_test.bash 0 vqa_lxr_results --test minival --load logs/vqa/vqa_lxr/BEST
# test
bash run/vqa_test.bash 0 vqa_src_base_results --test test --load logs/vqa/vqa_base/BEST.pth
bash run/vqa2_test.bash 0 vqa2_large_base_results --test test --load logs/vqa/vqa2_large_base/vqa2_large_base-Best.pth
bash run/vqa2_test.bash 0 vqa2_large_exchange_results --test test --load logs/vqa/vqa2_large_exchange/vqa2_large_exchange-Best.pth
bash run/vqa2_test.bash 0 vqa2_large_meanpooling_results --test test --load logs/vqa/vqa2_large_meanpooling/vqa2_large_meanpooling-Best.pth
bash run/vqa2_test.bash 0 vqa2_large_maxpooling_results --test test --load logs/vqa/vqa2_large_maxpooling/vqa2_large_maxpooling-Best.pth

# scp
scp -P 5102 zijie_xin@183.174.228.151:~/GeWu-Lab/LXMERT/logs/vqa/vqa_src_base_results/test_predict.json D:\2Codefield\VS_code\python\GeWuLab\LXMERT\logs\vqa\vqa_src_base_results


# tmux commands
tmux ls  # 查看当前 Tmux 会话
tmux new -s <session-name>  # 新建会话
tmux attach -t <session-name>  # 接入会话
# tmux attach -t res_only
# 分离回话！！！
tmux detach  # 可在命令行输入时
直接关闭窗口  # 模型运行，不可在命令行输入时
tmux kill-session -t <session-name>  # 杀死会话
tmux rename-session -t <old-name> <new-name>  # 重命名会话

# 杀内存
fuser -v /dev/nvidia*
kill -9 ...


LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers
------------------------------------------------------
BertConfig init
LXRTFeatureExtraction init
BertPreTrainedModel init
LXRTModel init
BertPreTrainedModel init
BertEmbeddings init

LXRTEncoder init
VisualFeatEncoder init

# 9 x :
BertLayer init
BertSelfattLayer init
BertAttention init
BertAttOutput init
BertIntermediate init
BertOutput init

# 5 x :
BertLayer init
BertSelfattLayer init
BertAttention init
BertAttOutput init
BertIntermediate init
BertOutput init


# 5 x :
LXRTXLayer init
BertCrossattLayer init
BertAttention init
BertAttOutput init
BertSelfattLayer init
BertAttention init
BertAttOutput init
BertSelfattLayer init
BertAttention init
BertAttOutput init
BertIntermediate init
BertOutput init
BertIntermediate init
BertOutput init

BertPooler init



在将多头注意力机制应用在多模态的方法中，由于无法确定Q的head_i与K的head_i交互最佳。我想通过可学习的maxpooling方式，确定Q的head_i与所有K的head分别做交互，然后计算他们相似度的最大值。类似于argmax_{k}(Q1K1, Q1K2, ..., Q1K_{head})，这样通过所有head的K获得一个与Q1相似度最大的相似度矩阵attention score。相当于学习Q点积K的方式来动态交互。
我想在下面的代码上做改变，你帮我写一下改进的代码
class BertDynamicAttention(nn.Module):
    def __init__(self, config, current_num_layers, ctx_dim=None):
        print("BertDynamicAttention init")
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads))
        self.current_num_layers = current_num_layers
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) # 768/12=64
        # all_head_size = hidden_size
        self.all_head_size = self.num_attention_heads * self.attention_head_size # 12*64=768

        # visual_dim = 2048
        if ctx_dim is None:
            ctx_dim =config.hidden_size
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(ctx_dim, self.all_head_size)
        self.value = nn.Linear(ctx_dim, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    # 拆分多头
    def transpose_for_scores(self, x):
        # [bs, seq_length, all_head_size] -> [bs, seq_length, num_attention_heads, attention_head_size]
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)  # 相当于reshape
        # [bs, seq_length, num_attention_heads, attention_head_size] -> [bs, num_attention_heads, seq_length, attention_head_size]
        x = x.transpose(1, 2) # x.permute(0, 2, 1, 3)
        return x

    def forward(self, hidden_states, context, attention_mask=None):
        # hidden_states -> query; context -> key, value
        # [bs, seq_length, v_hidden_size] -> [bs, seq_length, all_head_size]
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(context)
        mixed_value_layer = self.value(context)

        # [bs, seq_length, all_head_size] -> [bs, num_attention_heads, seq_length, attention_head_size]
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        # transpose: -> [bs, num_attention_heads, attention_head_size, seq_length]
        # @: multiply -> [bs, num_attention_heads, seq_length, seq_length]
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        # Normalize the attention scores to probabilities.
        attention_probs = nn.Softmax(dim=-1)(attention_scores)
        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs)

        # @: multiply -> [bs, num_attention_heads, seq_length, attention_head_size]
        context_layer = torch.matmul(attention_probs, value_layer)

        # 调换维度,准备合并多头
        # [bs, num_attention_heads, seq_length, attention_head_size] -> [bs, seq_length, num_attention_heads, attention_head_size]
        context_layer = context_layer.transpose(1, 2).contiguous()
        # context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        # 合并多头中的信息
        # [bs, seq_length, num_attention_heads, attention_head_size] -> [bs, seq_length, all_head_size]
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)  # 相当于reshape 
        return context_layer